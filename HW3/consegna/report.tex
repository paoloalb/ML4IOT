\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[document]{ragged2e}
\usepackage{todonotes}
\usepackage[a4paper, total={6.2in, 11in}]{geometry}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{tabularx,booktabs}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage{float}
\usepackage{tabularx}
    \newcolumntype{L}{>{\raggedright\arraybackslash}X}
    

\date{}
\begin{document}
\author{Paolo Aberto\\
StudentID : s278098\\

\and
Lorenzo De Nisi\\
Student ID: s276545\\

\and
Carmine De Stefano\\
Student ID: s278176\\
}

\justifying

\title{
    \vspace{0.8cm}
    
    Machine Learning for IoT - 
    Politecnico di Torino\\
    \vspace{.5cm}
    \Large \textbf{Homework 3 report}
    \vspace{.1cm}
}

\maketitle
\thispagestyle{empty} 
\vspace{-0.9cm}


\section{Big/Little Inference}
%spiegare che abbiamo scelto rest perchè è la cosa + logica con client-server

%uso della politica first-second (abbiamo provato entropoia ma era peggio)
%secondo - primo  > soglia
% citare policy entropia che non andava bene 

%little: stft per rispettare inference constraint. inserido dropout, cambiato policy

%big: MFCC to maximize acc. cambiato policy. aggiunto layer più grossi

%TABELLA CON RISULTATI PER I 2 MODELLI e hyperparameters

%Int16 invece di float32 perché codifichi meno byte. non inficia l'accuracy e ci ha permesso di ridurre il  costo nei limiti richiesti (quasi dimezzato costo)

\section{Cooperative Inference}
%giustificare uso di subscriber-publisher per via della presenza di n modelli
% mettete quality of service 0 per velicizzare (di default è 2)


% 4 modelli 2 cnn e 2 dscnn 

% TABELLA DA RIEMPIRE 

% per predizione finale mediamo i logits che escono dalla rete e facciamo argmax

% accuracy finale 95.125


\end{document}
