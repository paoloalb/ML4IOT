\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[document]{ragged2e}
\usepackage{todonotes}
\usepackage[a4paper, total={7in, 12in}]{geometry}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{tabularx,booktabs}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage{float}
\usepackage{tabularx}
    \newcolumntype{L}{>{\raggedright\arraybackslash}X}
    

\date{}
\begin{document}
\author{Paolo Aberto\\
StudentID : s278098\\

\and
Lorenzo De Nisi\\
Student ID: s276545\\

\and
Carmine De Stefano\\
Student ID: s278176\\
}

\justifying

\title{
    \vspace{0.4cm}
    
    Machine Learning for IoT - 
    Politecnico di Torino\\
    \vspace{.5cm}
    \Large \textbf{Homework 3 report}
    \vspace{.1cm}
}

\maketitle
\thispagestyle{empty} 
\vspace{-0.9cm}


\section{Big/Little Inference}
For the big/little inference, we chose to use perform it with the use of REST, since it was the best choice when working with a single client and a server.
When training the big model we used MFCC to maximize the accuracy, while we switched to STFT in the little one, in order to reduce significantly the inference time.
\\
For both networks, we decided to use a DS-CNN model, increasing the number of convolutional filters in the big model. For the small model, we also used magnitude based pruning, in order to respect thesize constraints while mantaining a good accuracy.\\
We also implemented learning rate policies for both the networks during training.\\\\
We tried different success checker policies, like a treshold on the entropy of the probabilities array. We finally settled on the first-second policy, where we used the difference between first and second biggest probabilities coming from the network prediction layer.
\\\\
When it comes to the communication cost, we managed to reduce it a lot by switching from Float32 to Int16 when sending the data to the server. This did not impact the resulting accuracy, while reducing the cost of communication by a significant margin and allowing us to call the server for a prediction 113 times, resulting in a comunnication cost of about 4.47 MB and a final accuracy of 93.12 \%.
\vspace{0.2cm}

\begin{center}
\begin{tabular}{ |c|c|c|c|c|c|c|c| } 
\hline
\vspace{-0.09cm}

\textbf{Model} &\textbf{Size} &\textbf{Compressed} &\textbf{Epochs}&\textbf{MFCC} &\textbf{Total}&\textbf{Test set}\\
\textbf{} &\textbf{} &\textbf{size} &\textbf{}&\textbf{}&\textbf{latency} &\textbf{accuracy}\\
\hline
Big   & 20 & & & YES & & 95.12\%\\
Little& 20 & & & NO  & & 90.62\%\\
\hline

\end{tabular}
\captionof{table}{Models used for the big/little inference} 

\end{center}


\section{Cooperative Inference}
For the cooperative inference, the chosen number of models is 4.\\
The communication is performed through MQTT, considering that it is relatively easy to publish the recording once for all the, possibly many, models with respect to create a web service for each of them.\\
We managed to keep the number of models as low as possible while preserving accuracy.
As expected all the models have an individual accuracy that is lower than the cooperative final accuracy that is 95.13\% .\\
Two of them are derived from the proposed DS-CNN, while the remaining two are derived from the proposed CNN, with some modification on the BatchNorm layer and filters and biases of the Conv2D.\\
To tackle the big amount of time needed to complete the 800 inferences, during testing the quality of service has been lowered to 0,
avoiding the four-step handshake that was time consuming. To ensure the correct communication in the final commit of the homework the
QOS is again 2 (but using the optional parameter \verb#qos# on both \verb#inference_client.py# and \verb#cooperative_client# is possible to change it)\\
The cooperative policy consists of averaging the logits (output of the last layer of the models) and taking the argmax of them.

\vspace{0.2cm}

\begin{center}
\begin{tabular}{ |c|c|c|c|c|c|c|c|c| } 
\hline
\vspace{-0.09cm}

\textbf{Model} &\textbf{Modification w.r.t.} &\textbf{Epochs}&\textbf{lr} &\textbf{Test set accuracy}\\
\textbf{} &\textbf{proposed models} &\textbf{}&\textbf{} &\textbf{}\\
\hline
CNN-0 &-& 20 & 0.01 & 94.25\\
\hline
CNN-1&Conv2D(filters=64, bias=True),&20&0.01&93.125\\
&BatchNormalization(momentum=0.2)&&&\\
\hline
DS-CNN-0&-&20&0.01&93.625\\
\hline
DS-CNN-1&Conv2D(filters=128, bias=True),&20&0.01&92.50\\
& BatchNormalization(momentum=0.2)&&&\\
\hline\hline
Cooperative inference &&&&\textbf{95.13} \\
\hline
\end{tabular}
\captionof{table}{Models used for the cooperative inference} 

\end{center}


\end{document}
