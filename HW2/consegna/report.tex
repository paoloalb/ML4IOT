\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[document]{ragged2e}
\usepackage{todonotes}
\usepackage[a4paper, total={6.2in, 12.5in}]{geometry}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{tabularx,booktabs}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage{float}
\usepackage{tabularx}
    \newcolumntype{L}{>{\raggedright\arraybackslash}X}
    
    

\date{}
\begin{document}
\author{Paolo Aberto\\
StudentID : s278098\\

\and
Lorenzo De Nisi\\
Student ID: s276545\\

\and
Carmine De Stefano\\
Student ID: s278176\\
}

\justifying

\title{
    \vspace{0.8cm}
    
    Machine Learning for IoT - 
    Politecnico di Torino\\
    \vspace{.5cm}
    \Large \textbf{Homework 2 report}
    \vspace{.1cm}
}

\maketitle
\thispagestyle{empty} 
\vspace{-0.9cm}


\section{Multi-Step Temperature and Humidity Forecasting}
After implementing a WindowGenerator class returning 6 output steps, we started to train the models. We settled on a weights-only quantization, and after tweaking the pruning parameters, we found two versions of the CNN model that were able to respect the given constraints.
The parameters changed in the 2 models were the final sparsity (\%) at which pruning ends, and the width multiplier $\alpha$.
\\\\
We adopted a callback function to stop the training when the validation set performances were not improving anymore. We adopted the standars Keras checkpoint approach, and we monitored the mean of the two MAE metrics for the checkpoint. 
\\\\
In the following table we can see the results for the two models, that were both trained for 20 epochs, with learning rate 0.001:

\vspace{0.2cm}

\begin{center}
\begin{tabular}{ |c|c|c|c|c|c|c|c| } 
\hline
\vspace{-0.09cm}

\textbf{Model} &\textbf{Quantization} & \textbf{Final} & \textbf{$\alpha$} &\textbf{Size} &\textbf{Compressed} &\textbf{Temp}&\textbf{Hum} \\
&&\textbf{sparsity}&&&\textbf{size}&\textbf{MAE}&\textbf{MAE}\\
\hline
A&Weights only & 0.745 & 0.12 & 5246 B & 1992 B & 0.4812 °C & 1.786 \% \\ 
B&Weights only & 0.75  & 0.07 & 4480 B & 1689 B & 0.4795 °C & 1.879 \% \\ 
\hline

\end{tabular}
\captionof{table}{Metrics for the final models obtained (version A and B)} 

\end{center}


\section{Keyword Spotting}

For keyword spotting on the mini speech command dataset, we used the DS-CNN model with weights-only quantization, using the following hyperparameters:

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\vspace{-0.1cm}

\textbf{Model} &\textbf{Quantization} &\textbf{MFCC} &\textbf{Final} & \textbf{$\alpha$} &\textbf{Epochs} &\textbf{Learning} \\

&&&\textbf{sparsity}&&&\textbf{Rate} \\
\hline
A & Weights only & YES &0.9 & 0.7  & 20  & 0.0075\\
B& Weights only & YES &- &  0.3   & 30 & 0.005\\ 
C & Weights only & NO &- &  0.4   & 30 & 0.01\\ 

\hline
\end{tabular}
\captionof{table}{Hyperparameters for the final models  (version A, B and C)} 
\end{center}
Like before, we adopted a callback function to monitor the accuracy metric on the validation set.
In order to obtain the performances requested in version B, we used a quite low parameter $\alpha$, to reduce inference computation. For version C instead, we switched to STFT in order to dramatically reduce processing time.
STFT and MFCC were computed with the same parameter used in the labs.
\\\\
The final results are shown in the table:
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\vspace{-0.1cm}

\textbf{Model} &\textbf{Size} &\textbf{Compr.}&  \textbf{Inference }& \textbf{Total}  &\textbf{Test} \\
&&\textbf{size}&\textbf{Latency}&\textbf{Latency}&\textbf{Accuracy}\\
\hline
A  & 77248 B & 21764 B & 9.03ms&   66.71ms  & 0.91375 \\ 
B  & 26960 B & 23233 B  &  1.16ms& 58.72ms & 0.91 \\ 
C  & 40320 B & 34831 B  & 4.52ms & 21.07ms  & 0.9175 \\ 

\hline
\end{tabular}
\captionof{table}{Metrics for the final models (version A, B and C)} 
\end{center}
We noticed that on different OS or different PCs, the way python lists the directories changes. This means that the order of the labels could change if it is obtained directly from the directory names.
To solve this issue, the order was fixed with a list.
\\
For both models, compression was performed thanks to the zlib library. Final memory performances were calculated by considering the compressed model.
For both tasks, we used the original architectures provided in the laboratory.

\end{document}
